---
title: DSP Observability
description: Observability tools and practices for the Digital Solutions Platform (DSP)
---

[&larr; back to Overview](/dsp)

# Observability

This page provides an overview of the observability tools and practices used in the Digital Solutions Platform (DSP).
Observability is a critical aspect of modern software development and operations, enabling teams to monitor, troubleshoot, and optimize their applications effectively.
It encompasses various tools and practices that provide insights into the performance, health, and behavior of applications and infrastructure.
More details about the internals of the observability setup can be found on the [Observability with LGTM+](https://groupspace.vaillant-group.com/x/Ha99Hg) Confluence page.

---

## Observability Tools
The DSP employs a range of observability tools to ensure comprehensive monitoring and analysis of applications and infrastructure. 
We based our solution on LGTM stack (Loki, Grafana, Tempo, and Mimir) to provide a unified observability experience. The open-source LGTM stack is designed to work seamlessly together, providing a cohesive solution for monitoring and analyzing applications.
Our current setup includes tools like:
- **Grafana Loki**: A horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus. Loki is designed to be cost-effective and easy to operate, making it an ideal choice for log management in the DSP. It does not index the contents of the logs, but rather a set of labels for each log stream.
- **Grafana**: An open-source analytics and monitoring platform that integrates with various data sources, including Prometheus. Grafana provides a rich set of visualization options, allowing users to create interactive dashboards and alerts based on the collected metrics.
- **Grafana Alloy**: Grafana Alloy is a flexible, high performance, vendor-neutral distribution of the OpenTelemetry Collector. Itâ€™s fully compatible with the most popular open source observability standards such as OpenTelemetry and Prometheus.
- **Azure Storage**: Azure Storage is used for storing logs and other data generated by the observability tools. It provides a scalable and secure storage solution, ensuring that logs are retained for analysis and troubleshooting.

---

# Platform Observability Architecture

## Overview

This document describes the observability setup for the platform and tenant environments. The architecture is designed for secure, multi-tenant operation with clear separation of log data between the platform (hub) and client (spoke) clusters, and between individual tenant teams.

---

## Components

### 1. Platform Hub Cluster

- **Purpose:** Central management cluster for the platform team.
- **Grafana Loki:** Deployed in the hub cluster, using Azure Storage as the backend for log storage.
- **Grafana Alloy:** Collects Kubernetes logs from the hub cluster and sends them to Loki.
- **Grafana:** Hosted on the hub cluster, used for viewing logs collected from the hub cluster and cluster level logs from spoke clusters.

### 2. Tenant Spoke Clusters

- **Spoke Clusters:** Each tenant (client) has one or more clusters, typically one per environment (e.g., dev, staging, prod).
- **Namespaces:** Each team within a spoke cluster operates in its own namespace.
- **Grafana Alloy:** Provisioned by default on all spoke clusters; collects logs from all namespaces and system components.
- **Log Collection:** Alloy sends logs to central Loki instance. The logs are organized by tenant/team.
- **Team Grafana:** Each team has its own Grafana instance (hosted on the hub cluster, accessible behind VPN), with access restricted to logs from their own namespace.

### 3. Azure Storage

- **Log Storage:** All logs are stored in Azure Blob Storage.
- **Separation:** Logs for each team are stored in separate blobs, ensuring strict isolation between tenants and teams.

---

## Log Flow

### Hub Cluster Log Flow

1. **Grafana Alloy** collects all Kubernetes logs from the platform hub cluster.
2. Logs are sent to **Grafana Loki** (deployed in the hub cluster).
3. Loki stores logs in **Azure Blob Storage**.
4. Logs are visualized in **Platform Team Grafana **.

### Spoke Cluster Log Flow

1. **Grafana Alloy** is deployed on every spoke cluster, collecting logs from all namespaces.
2. Logs are sent to central Loki instance, which then send them to Azure Blob Storage, with each team's logs stored in their respective blobs.
3. **Platform Team Grafana** can only access and display cluster-level logs (excluding team namespaces).
4. Each team has a dedicated **Grafana instance** (hosted on the hub cluster), which provides access only to logs from their own namespaces.

---

## Access Control & Security

- **Platform Team:** 
    - Full visibility into all logs from the hub cluster and cluster-level logs from spoke clusters via **Platform Team Grafana** instance. 
    - No access to team/application logs.
- **Tenant Teams (Spoke Clusters):**
    - Teams can access logs from their own namespaces and cluster-level logs via their dedicated Grafana instance.
    - No cross-team log access is possible; access is enforced at the namespace and storage blob level using built-in multi tenancy mechanism.
    - Grafana instance is secured behind VPN access, ensuring that only authorized users can view logs.
